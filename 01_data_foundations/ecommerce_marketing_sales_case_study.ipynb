{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380cf122",
   "metadata": {},
   "source": [
    "## Page 1\n",
    "\n",
    "!gdown--folderhttps://drive.google.com/drive/folders/1Qt1HfSoTyCKiyDy2frR -hYOT9UvfwGq7\n",
    "Retrieving folder contents\n",
    "Processing file 1rqihT647UW9HEmERxCrTJrcZZ9xE5TEi CustomersData.xlsx\n",
    "Processing file 1u2gedRtVaCqTQNylaohI9WO6m3pA8rnc Dataset Description.docx\n",
    "Processing file 1CfORUKckP7Qi9swmCe8XEgLLWnpKl4eE Discount_Coupon.csv\n",
    "Processing file 1hjHgCluvPEUfrp9w-_-73ch0d9SK6FHf Marketing_Spend.csv\n",
    "Processing file 1ZvQn7-UtGSdJa3H9zM4ve_uzACFAzF0Z Online_Sales.csv\n",
    "Processing file 1tqJmtKcfhiEJXYVV21ybnD4UaXYW9r4v Tax_amount.xlsx\n",
    "Retrieving folder contents completed\n",
    "Building directory structure\n",
    "Building directory structure completed\n",
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1rqihT647UW9HEmERxCrTJrcZZ9xE5TEi\n",
    "To: /content/DAV Business case/CustomersData.xlsx\n",
    "100% 42.2k/42.2k [00:00<00:00, 120MB/s]\n",
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1u2gedRtVaCqTQNylaohI9WO6m3pA8rnc\n",
    "To: /content/DAV Business case/Dataset Description.docx\n",
    "100% 9.06k/9.06k [00:00<00:00, 36.8MB/s]\n",
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1CfORUKckP7Qi9swmCe8XEgLLWnpKl4eE\n",
    "To: /content/DAV Business case/Discount_Coupon.csv\n",
    "100% 4.92k/4.92k [00:00<00:00, 24.2MB/s]\n",
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1hjHgCluvPEUfrp9w-_-73ch0d9SK6FHf\n",
    "To: /content/DAV Business case/Marketing_Spend.csv\n",
    "100% 8.67k/8.67k [00:00<00:00, 37.1MB/s]\n",
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1ZvQn7-UtGSdJa3H9zM4ve_uzACFAzF0Z\n",
    "To: /content/DAV Business case/Online_Sales.csv\n",
    "100% 5.24M/5.24M [00:00<00:00, 35.3MB/s]\n",
    "Downloading...\n",
    "From: https://drive.google.com/uc?id=1tqJmtKcfhiEJXYVV21ybnD4UaXYW9r4v\n",
    "To: /content/DAV Business case/Tax_amount.xlsx\n",
    "100% 9.84k/9.84k [00:00<00:00, 48.2MB/s]\n",
    "Download completed\n",
    "import pandasaspd\n",
    "online_sales_df =pd.read_csv ('DAV Business case/Online_Sales.csv' )\n",
    "display(online_sales_df .head())\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a69c4",
   "metadata": {},
   "source": [
    "## Page 2\n",
    "\n",
    "CustomerID Transaction_ID Transaction_Date Product_SK UProduct_Description\n",
    "0 17850 16679 1/1/2019 GGOENEBJ079499Nest L ear\n",
    "Thermostat 3r d Gen-\n",
    "USA - Stainle...\n",
    "1 17850 16680 1/1/2019 GGOENEBJ079499Nest L ear\n",
    "Thermostat 3r d Gen-\n",
    "USA - Stainle...\n",
    "2 17850 16681 1/1/2019 GGOEGFKQ020399Google Laptop and\n",
    "Cell Phone Stick\n",
    "3 17850 16682 1/1/2019 GGOEGA AB010516Google Men's 100%\n",
    "Cotton Short Sleeve\n",
    "Hero T\n",
    "4 17850 16682 1/1/2019 GGOEGBJL013999Google Canvas T\n",
    "Natural/Navy\n",
    "display(tax_amount_df .info())\n",
    "display(tax_amount_df .isnull().sum())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 20 entries, 0 to 19\n",
    "Data columns (total 2 columns):\n",
    "#   Column            Non-Null Count  Dtype\n",
    "---  ------            --------------  -----\n",
    "0   Product_Category  20 non-null     object\n",
    "1   GST               20 non-null     float64\n",
    "dtypes: float64(1), object(1)\n",
    "memory usage: 452.0+ bytes\n",
    "None\n",
    "0\n",
    "Product_Category 0\n",
    "GST 0\n",
    "dtype: int64\n",
    "display(marketing_spend_df .info())\n",
    "display(marketing_spend_df .isnull().sum())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 365 entries, 0 to 364\n",
    "Data columns (total 3 columns):\n",
    "#   Column         Non-Null Count  Dtype\n",
    "---  ------         --------------  -----\n",
    "0   Date           365 non-null    object\n",
    "1   Offline_Spend  365 non-null    int64\n",
    "2   Online_Spend   365 non-null    float64\n",
    "dtypes: float64(1), int64(1), object(1)\n",
    "memory usage: 8.7+ KB\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19575851",
   "metadata": {},
   "source": [
    "## Page 3\n",
    "\n",
    "None\n",
    "0\n",
    "Date 0\n",
    "Offline_Spend 0\n",
    "Online_Spend 0\n",
    "dtype: int64\n",
    "display(discount_coupon_df .info())\n",
    "display(discount_coupon_df .isnull().sum())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 204 entries, 0 to 203\n",
    "Data columns (total 4 columns):\n",
    "#   Column            Non-Null Count  Dtype\n",
    "---  ------            --------------  -----\n",
    "0   Month             204 non-null    object\n",
    "1   Product_Category  204 non-null    object\n",
    "2   Coupon_Code       204 non-null    object\n",
    "3   Discount_pct      204 non-null    int64\n",
    "dtypes: int64(1), object(3)\n",
    "memory usage: 6.5+ KB\n",
    "None\n",
    "0\n",
    "Month 0\n",
    "Product_Category 0\n",
    "Coupon_Code 0\n",
    "Discount_pct 0\n",
    "dtype: int64\n",
    "display(customers_df .info())\n",
    "display(customers_df .isnull().sum())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1468 entries, 0 to 1467\n",
    "Data columns (total 4 columns):\n",
    "#   Column         Non-Null Count  Dtype\n",
    "---  ------         --------------  -----\n",
    "0   CustomerID     1468 non-null   int64\n",
    "1   Gender         1468 non-null   object\n",
    "2   Location       1468 non-null   object\n",
    "3   Tenure_Months  1468 non-null   int64\n",
    "dtypes: int64(2), object(2)\n",
    "memory usage: 46.0+ KB\n",
    "None\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fec402",
   "metadata": {},
   "source": [
    "## Page 4\n",
    "\n",
    "0\n",
    "CustomerID 0\n",
    "Gender 0\n",
    "Location 0\n",
    "Tenure_Months 0\n",
    "dtype: int64\n",
    "display(online_sales_df .info())\n",
    "display(online_sales_df .isnull().sum())\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 52924 entries, 0 to 52923\n",
    "Data columns (total 10 columns):\n",
    "#   Column               Non-Null Count  Dtype\n",
    "---  ------               --------------  -----\n",
    "0   CustomerID           52924 non-null  int64\n",
    "1   Transaction_ID       52924 non-null  int64\n",
    "2   Transaction_Date     52924 non-null  object\n",
    "3   Product_SKU          52924 non-null  object\n",
    "4   Product_Description  52924 non-null  object\n",
    "5   Product_Category     52924 non-null  object\n",
    "6   Quantity             52924 non-null  int64\n",
    "7   Avg_Price            52924 non-null  float64\n",
    "8   Delivery_Charges     52924 non-null  float64\n",
    "9   Coupon_Status        52924 non-null  object\n",
    "dtypes: float64(2), int64(3), object(5)\n",
    "memory usage: 4.0+ MB\n",
    "None\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f337f",
   "metadata": {},
   "source": [
    "## Page 5\n",
    "\n",
    "0\n",
    "CustomerID 0\n",
    "Transaction_ID 0\n",
    "Transaction_Date 0\n",
    "Product_SK U0\n",
    "Product_Description 0\n",
    "Product_Category 0\n",
    "Quantity 0\n",
    "Avg_Price 0\n",
    "Delivery_Charges 0\n",
    "Coupon_Status 0\n",
    "dtype: int64\n",
    "import pandasaspd\n",
    "marketing_spend_df =pd.read_csv ('DAV Business case/Marketing_Spend.csv' )\n",
    "display(marketing_spend_df .head())\n",
    "Date Offline_Spend Online_Spend\n",
    "01/1/2019 4500 2424.50\n",
    "11/2/2019 4500 3480.36\n",
    "21/3/2019 4500 1576.38\n",
    "31/4/2019 4500 2928.55\n",
    "41/5/2019 4500 4055.30\n",
    "import pandasaspd\n",
    "tax_amount_df =pd.read_excel ('DAV Business case/Tax_amount.xlsx' )\n",
    "display(tax_amount_df .head())\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31527251",
   "metadata": {},
   "source": [
    "## Page 6\n",
    "\n",
    "Product_Category GST\n",
    "0 Nest-US A0.10\n",
    "1 Office 0.10\n",
    "2 Appar el0.18\n",
    "3 Bags 0.18\n",
    "4 Drinkwar e0.18\n",
    "import pandasaspd\n",
    "discount_coupon_df =pd.read_csv ('DAV Business case/Discount_Coupon.csv' )\n",
    "display(discount_coupon_df .head())\n",
    "Month Product_Category Coupon_Code Discount_pct\n",
    "0 Jan Appar el SALE10 10\n",
    "1 Feb Appar el SALE20 20\n",
    "2 Mar Appar el SALE30 30\n",
    "3 Jan Nest-US A ELEC10 10\n",
    "4 Feb Nest-US A ELEC20 20\n",
    "import pandasaspd\n",
    "customers_df =pd.read_excel ('DAV Business case/CustomersData.xlsx' )\n",
    "display(customers_df .head())\n",
    "CustomerID Gender Location Tenure_Months\n",
    "0 17850 M Chicago 12\n",
    "1 13047 MCalifor nia 43\n",
    "2 12583 M Chicago 33\n",
    "3 13748 FCalifor nia 30\n",
    "4 15100 MCalifor nia 49\n",
    "Business question 1\n",
    "Analyze the data to identif y months with the highest and lowestcustomer\n",
    "acquisition count and suggest strategies.\n",
    "online_sales_df ['Transaction_Date' ]=pd.to_datetime (online_sales_df ['Transaction_Date'\n",
    "In[]:\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbefb8",
   "metadata": {},
   "source": [
    "## Page 7\n",
    "\n",
    "first_purchase_df =online_sales_df .groupby('CustomerID' )['Transaction_Date' ].min\n",
    "first_purchase_df ['Acquisition_Month' ]=first_purchase_df ['Transaction_Date' ].\n",
    "monthly_acquisition_counts =first_purchase_df ['Acquisition_Month' ].value_counts\n",
    "highest_acquisition_month =monthly_acquisition_counts .idxmax()\n",
    "lowest_acquisition_month =monthly_acquisition_counts .idxmin()\n",
    "print(\"Monthly Customer Acquisition Counts:\" )\n",
    "display(monthly_acquisition_counts )\n",
    "print(f\"\\nMonth with Highest Acquisition: Month {highest_acquisition_month }({monthly_acquisition_counts\n",
    "print(f\"Month with Lowest Acquisition: Month {lowest_acquisition_month }({monthly_acquisition_counts\n",
    "print(\"\\nStrategies:\" )\n",
    "print(f\"To leverage the high-performing month (Month {highest_acquisition_month\n",
    "print(\"- Increasing marketing spend and promotional activities during this month.\"\n",
    "print(\"- Stocking up on popular products or launching new products to capitalize on the higher traffic.\"\n",
    "print(\"- Running targeted campaigns to encourage repeat purchases from newly acquired customers.\"\n",
    "print(f\"\\nTo improve acquisition during the slower month (Month {lowest_acquisition_month\n",
    "print(\"- Offering special discounts or promotions to incentivize new customers.\"\n",
    "print(\"- Exploring new marketing channels or strategies that might be more effective during this period.\"\n",
    "print(\"- Analyzing customer behavior in this month to understand why acquisition is lower and address those factors.\"\n",
    "Monthly Customer Acquisition Counts:\n",
    "count\n",
    "Acquisition_Month\n",
    "1 215\n",
    "2 96\n",
    "3 177\n",
    "4 163\n",
    "5 112\n",
    "6 137\n",
    "7 94\n",
    "8 135\n",
    "9 78\n",
    "10 87\n",
    "11 68\n",
    "12 106\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7ccd9",
   "metadata": {},
   "source": [
    "## Page 8\n",
    "\n",
    "Month with Highest Acquisition: Month 1 (215 customers)\n",
    "Month with Lowest Acquisition: Month 11 (68 customers)\n",
    "Strategies:\n",
    "To leverage the high-performing month (Month 1), consider:\n",
    "- Increasing marketing spend and promotional activities during this month.\n",
    "- Stocking up on popular products or launching new products to capitalize on th\n",
    "e higher traffic.\n",
    "- Running targeted campaigns to encourage repeat purchases from newly acquired\n",
    "customers.\n",
    "To improve acquisition during the slower month (Month 11), consider:\n",
    "- Offering special discounts or promotions to incentivize new customers.\n",
    "- Exploring new marketing channels or strategies that might be more effective d\n",
    "uring this period.\n",
    "- Analyzing customer behavior in this month to understand why acquisition is lo\n",
    "wer and address those factors.\n",
    "Business question 2\n",
    "Analyze if certain months consistently show higher or lower acquisition and\n",
    "suggest strategies to capitalize on high-perfor ming months and impr ove slower\n",
    "periods.\n",
    "import matplotlib.pyplot asplt\n",
    "print(\"Analyzing monthly acquisition patterns and suggesting strategies:\" )\n",
    "# Plot the monthly acquisition counts\n",
    "plt.figure(figsize=(10,6))\n",
    "monthly_acquisition_counts .plot(kind='bar')\n",
    "plt.title('Monthly Customer Acquisition Counts' )\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of New Customers' )\n",
    "plt.xticks(rotation =0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout ()\n",
    "plt.show()\n",
    "# Identify consistent patterns\n",
    "# We can visually inspect the bar plot for consistent high/low months.\n",
    "# For a more programmatic approach, we could look at consecutive months or compare to the average.\n",
    "# Based on the previous output and plot, January has the highest acquisition and November has the lowest.\n",
    "print(\"\\nStrategies to capitalize on high-performing months (e.g., January):\" )\n",
    "print(\"- Intensify marketing campaigns and promotions.\" )\n",
    "print(\"- Ensure sufficient inventory of popular products.\" )\n",
    "print(\"- Analyze successful strategies used in these months and replicate them.\"\n",
    "print(\"- Offer loyalty programs to new customers acquired during peak months to encourage retention.\"\n",
    "print(\"\\nStrategies to improve performance during slower months (e.g., November):\"\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c576c29",
   "metadata": {},
   "source": [
    "## Page 9\n",
    "\n",
    "print(\"- Launch targeted marketing campaigns with special offers or discounts.\"\n",
    "print(\"- Explore new customer acquisition channels.\" )\n",
    "print(\"- Conduct market research to understand customer behavior and preferences during these months.\"\n",
    "print(\"- Collaborate with partners for cross-promotional activities.\" )\n",
    "print(\"- Optimize online advertising spend and targeting.\" )\n",
    "Analyzing monthly acquisition patterns and suggesting strategies:\n",
    "Strategies to capitalize on high-performing months (e.g., January):\n",
    "- Intensify marketing campaigns and promotions.\n",
    "- Ensure sufficient inventory of popular products.\n",
    "- Analyze successful strategies used in these months and replicate them.\n",
    "- Offer loyalty programs to new customers acquired during peak months to encour\n",
    "age retention.\n",
    "Strategies to improve performance during slower months (e.g., November):\n",
    "- Launch targeted marketing campaigns with special offers or discounts.\n",
    "- Explore new customer acquisition channels.\n",
    "- Conduct market research to understand customer behavior and preferences durin\n",
    "g these months.\n",
    "- Collaborate with partners for cross-promotional activities.\n",
    "- Optimize online advertising spend and targeting.\n",
    "Business question 3\n",
    "Identif y periods with the str ongest and weak est retention rates and suggest\n",
    "strategies to impr ove r etention during weak er months.\n",
    "online_sales_df ['Transaction_Date' ]=pd.to_datetime (online_sales_df ['Transaction_Date'\n",
    "online_sales_df ['Invoice_Month' ]=online_sales_df ['Transaction_Date' ].dt.to_period\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e249705",
   "metadata": {},
   "source": [
    "## Page 10\n",
    "\n",
    "customer_monthly_purchase =online_sales_df .groupby(['CustomerID' ,'Invoice_Month'\n",
    "customer_monthly_purchase ['Has_Purchased' ]=1\n",
    "monthly_purchase_pivot =customer_monthly_purchase .pivot_table (\n",
    "index='CustomerID' ,\n",
    "columns='Invoice_Month' ,\n",
    "values='Has_Purchased'\n",
    ").fillna(0)\n",
    "display(monthly_purchase_pivot .head())\n",
    "Invoice_Month 2019-01 2019-02 2019-03 2019-04 2019-05 2019-06 2019-07 2019-08\n",
    "CustomerID\n",
    "12346 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
    "12347 0.0 0.0 1.0 0.0 0.0 0.0 0.0\n",
    "12348 0.0 0.0 0.0 0.0 0.0 1.0 0.0\n",
    "12350 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
    "12356 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
    "retention_rates ={}\n",
    "months=monthly_purchase_pivot .columns\n",
    "for iinrange(len(months)-1):\n",
    "current_month =months[i]\n",
    "next_month =months[i+1]\n",
    "customers_current_month =monthly_purchase_pivot [monthly_purchase_pivot [current_month\n",
    "customers_next_month =monthly_purchase_pivot [monthly_purchase_pivot [next_month\n",
    "retained_customers =customers_current_month .intersection (customers_next_month\n",
    "iflen(customers_current_month )>0:\n",
    "retention_rate =len(retained_customers )/len(customers_current_month )\n",
    "retention_rates [next_month ]=retention_rate\n",
    "else :\n",
    "retention_rates [next_month ]=0.0\n",
    "retention_series =pd.Series(retention_rates )\n",
    "highest_retention_month =retention_series .idxmax()\n",
    "lowest_retention_month =retention_series .idxmin()\n",
    "print(\"Monthly Retention Rates:\" )\n",
    "display(retention_series )\n",
    "print(f\"\\nMonth with Highest Retention: {highest_retention_month }({retention_series\n",
    "print(f\"Month with Lowest Retention: {lowest_retention_month }({retention_series\n",
    "Monthly Retention Rates:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3805b",
   "metadata": {},
   "source": [
    "## Page 11\n",
    "\n",
    "0\n",
    "2019-02 0.060465\n",
    "2019-03 0.100917\n",
    "2019-04 0.115385\n",
    "2019-05 0.111607\n",
    "2019-06 0.185000\n",
    "2019-07 0.223938\n",
    "2019-08 0.275424\n",
    "2019-09 0.146667\n",
    "2019-10 0.150259\n",
    "2019-11 0.147619\n",
    "2019-12 0.148936\n",
    "dtype: float64\n",
    "Month with Highest Retention: 2019-08 (0.28)\n",
    "Month with Lowest Retention: 2019-02 (0.06)\n",
    "print(\"\\nStrategies to improve retention during weaker months (e.g., February, which had the lowest retention):\"\n",
    "print(\"- Implement targeted re-engagement campaigns for customers who did not make a purchase in the subsequent month after their initial purchase in a low-retention month.\"\n",
    "print(\"- Offer special discounts or exclusive offers to customers who purchased in a weaker retention month but haven't returned.\"\n",
    "print(\"- Collect feedback from customers who churned during these periods to understand the reasons and address pain points.\"\n",
    "print(\"- Improve customer service and support during these months to ensure a positive experience.\"\n",
    "print(\"- Analyze the products purchased in weaker retention months and identify if there are issues with those products or their descriptions.\"\n",
    "print(\"- Consider loyalty programs or subscription models to encourage repeat purchases.\"\n",
    "print(\"- Personalize marketing communications based on customer purchase history and preferences.\"\n",
    "Strategies to improve retention during weaker months (e.g., February, which had\n",
    "the lowest retention):\n",
    "- Implement targeted re-engagement campaigns for customers who did not make a p\n",
    "urchase in the subsequent month after their initial purchase in a low-retention\n",
    "month.\n",
    "- Offer special discounts or exclusive offers to customers who purchased in a w\n",
    "eaker retention month but haven't returned.\n",
    "- Collect feedback from customers who churned during these periods to understan\n",
    "d the reasons and address pain points.\n",
    "- Improve customer service and support during these months to ensure a positive\n",
    "experience.\n",
    "- Analyze the products purchased in weaker retention months and identify if the\n",
    "re are issues with those products or their descriptions.\n",
    "- Consider loyalty programs or subscription models to encourage repeat purchase\n",
    "s.\n",
    "- Personalize marketing communications based on customer purchase history and p\n",
    "references.\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601cc3c",
   "metadata": {},
   "source": [
    "## Page 12\n",
    "\n",
    "Business question 4\n",
    "Analyze customer behavior during high-r etention months and suggest ways to\n",
    "replicate this success.\n",
    "# Filter transactions for the highest retention month (August 2019)\n",
    "highest_retention_month_data =online_sales_df [online_sales_df ['Invoice_Month' ]\n",
    "print(\"Analysis of Customer Behavior in August 2019 (Highest Retention Month):\"\n",
    "# Analyze Product Category distribution\n",
    "print(\"\\nProduct Category Distribution:\" )\n",
    "display(highest_retention_month_data ['Product_Category' ].value_counts (normalize\n",
    "# Analyze Avg_Price distribution (using describe for summary statistics)\n",
    "print(\"\\nAverage Price Distribution:\" )\n",
    "display(highest_retention_month_data ['Avg_Price' ].describe ())\n",
    "# Analyze Coupon_Status distribution\n",
    "print(\"\\nCoupon Status Distribution:\" )\n",
    "display(highest_retention_month_data ['Coupon_Status' ].value_counts (normalize =True\n",
    "# Analyze average Quantity purchased\n",
    "print(\"\\nAverage Quantity Purchased:\" )\n",
    "display(highest_retention_month_data ['Quantity' ].mean())\n",
    "# Analyze Delivery_Charges distribution (using describe for summary statistics)\n",
    "print(\"\\nDelivery Charges Distribution:\" )\n",
    "display(highest_retention_month_data ['Delivery_Charges' ].describe ())\n",
    "Analysis of Customer Behavior in August 2019 (Highest Retention Month):\n",
    "Product Category Distribution:\n",
    "proportion\n",
    "Product_Category\n",
    "Apparel 0.458537\n",
    "Nest-USA 0.176911\n",
    "Office 0.087967\n",
    "Drinkware 0.075122\n",
    "Lifestyle 0.069756\n",
    "dtype: float64\n",
    "Average Price Distribution:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c5717",
   "metadata": {},
   "source": [
    "## Page 13\n",
    "\n",
    "Avg_Price\n",
    "count 6150.000000\n",
    "mean 34.743348\n",
    "std 49.353248\n",
    "min 0.790000\n",
    "25% 4.800000\n",
    "50% 11.990000\n",
    "75% 31.990000\n",
    "max 249.000000\n",
    "dtype: float64\n",
    "Coupon Status Distribution:\n",
    "proportion\n",
    "Coupon_Status\n",
    "Click ed 0.505528\n",
    "Used 0.340325\n",
    "Not Used 0.154146\n",
    "dtype: float64\n",
    "Average Quantity Purchased:\n",
    "np.float64(4.4645528455284555)\n",
    "Delivery Charges Distribution:\n",
    "Delivery_Charges\n",
    "count 6150.000000\n",
    "mean 9.934889\n",
    "std 17.211588\n",
    "min 0.000000\n",
    "25% 6.000000\n",
    "50% 6.000000\n",
    "75% 6.000000\n",
    "max 521.360000\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ad209",
   "metadata": {},
   "source": [
    "## Page 14\n",
    "\n",
    "print(\"\\nStrategies to replicate success of high-retention months (based on August 2019 analysis):\"\n",
    "print(\"- Focus marketing and promotions on the most popular product categories identified (Apparel, Nest-USA, Office, Drinkware, Lifestyle). Consider increasing inventory and visibility for these items in other months.\"\n",
    "print(\"- Leverage coupon usage as a retention driver. The high rate of 'Clicked' and 'Used' coupons suggests they are effective. Consider running targeted coupon campaigns in other months, potentially offering different discount levels or focusing on different product categories.\"\n",
    "print(\"- Analyze the average price and delivery charge distributions in August compared to lower retention months. If there are significant differences, consider adjusting pricing strategies or offering free/reduced delivery charges during weaker retention periods.\"\n",
    "print(\"- Promote products that are frequently purchased in higher quantities, as the average quantity is relatively high in August. Bundle deals or multi-buy offers could encourage this in other months.\"\n",
    "print(\"- Investigate the specific marketing campaigns or external factors that were active in August 2019 to understand if they contributed to the high retention rate and if they can be replicated.\"\n",
    "Strategies to replicate success of high-retention months (based on August 2019\n",
    "analysis):\n",
    "- Focus marketing and promotions on the most popular product categories identif\n",
    "ied (Apparel, Nest-USA, Office, Drinkware, Lifestyle). Consider increasing inve\n",
    "ntory and visibility for these items in other months.\n",
    "- Leverage coupon usage as a retention driver. The high rate of 'Clicked' and\n",
    "'Used' coupons suggests they are effective. Consider running targeted coupon ca\n",
    "mpaigns in other months, potentially offering different discount levels or focu\n",
    "sing on different product categories.\n",
    "- Analyze the average price and delivery charge distributions in August compare\n",
    "d to lower retention months. If there are significant differences, consider adj\n",
    "usting pricing strategies or offering free/reduced delivery charges during weak\n",
    "er retention periods.\n",
    "- Promote products that are frequently purchased in higher quantities, as the a\n",
    "verage quantity is relatively high in August. Bundle deals or multi-buy offers\n",
    "could encourage this in other months.\n",
    "- Investigate the specific marketing campaigns or external factors that were ac\n",
    "tive in August 2019 to understand if they contributed to the high retention rat\n",
    "e and if they can be replicated.\n",
    "Business question 5\n",
    "Compar e the r evenue generated by new and e xisting customers month-over -month\n",
    "and interpr et the tr end r egarding acquisition and r etention efforts.\n",
    "merged_df =pd.merge(online_sales_df ,first_purchase_df [['CustomerID' ,'Transaction_Date'\n",
    "merged_df ['Transaction_Month' ]=merged_df ['Transaction_Date_transaction' ].dt.to_period\n",
    "merged_df ['Acquisition_Month_Period' ]=merged_df ['Transaction_Date_acquisition'\n",
    "merged_df ['Customer_Type' ]=merged_df .apply(lambda row:'New'ifrow['Transaction_Month'\n",
    "# Merge with discount and tax dataframes\n",
    "merged_df =pd.merge(merged_df ,discount_coupon_df [['Product_Category' ,'Discount_pct'\n",
    "merged_df ['Discount_pct' ]=merged_df ['Discount_pct' ].fillna(0)/100.0\n",
    "merged_df =pd.merge(merged_df ,tax_amount_df ,on='Product_Category' ,how='left'\n",
    "merged_df ['GST']=merged_df ['GST'].fillna(0)\n",
    "# Calculate Invoice Value, handling potential issues with Avg_Price, Quantity, Delivery_Charges\n",
    "merged_df ['Invoice_Value' ]=((merged_df ['Quantity' ]*merged_df ['Avg_Price' ]).\n",
    "monthly_revenue =merged_df .groupby(['Transaction_Month' ,'Customer_Type' ])['Invoice_Value'\n",
    "monthly_revenue_pivot =monthly_revenue .pivot_table (index='Transaction_Month' ,\n",
    "display(monthly_revenue_pivot .head())\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac431f16",
   "metadata": {},
   "source": [
    "## Page 15\n",
    "\n",
    "Customer_T ype Existing New\n",
    "Transaction_Month\n",
    "2019-01 0.00000 463883.05705\n",
    "2019-02 41053.61768 286842.94252\n",
    "2019-03 57181.70852 279623.49531\n",
    "2019-04 183188.12296 264811.07227\n",
    "2019-05 115067.71388 203488.58668\n",
    "import matplotlib.pyplot asplt\n",
    "plt.figure(figsize=(12,6))\n",
    "monthly_revenue_pivot .plot(kind='line',marker='o',ax=plt.gca())\n",
    "plt.title('Monthly Revenue by Customer Type (New vs. Existing)' )\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Revenue' )\n",
    "plt.xticks(rotation =45)\n",
    "plt.grid(True )\n",
    "plt.tight_layout ()\n",
    "plt.show()\n",
    "print(\"Interpretation of Monthly Revenue Trend:\" )\n",
    "print(\"- Initially, the revenue is entirely driven by new customers, as expected in the first month of data.\"\n",
    "print(\"- Over time, the revenue from existing customers grows steadily, indicating successful customer retention and repeat purchases.\"\n",
    "print(\"- Revenue from new customers fluctuates month-to-month but generally contributes a significant portion of the total revenue.\"\n",
    "print(\"- The increasing contribution of existing customers to the overall revenue suggests that retention efforts are having a positive impact.\"\n",
    "print(\"- The balance between acquisition and retention seems to be shifting towards a healthier mix as the year progresses, with existing customers becoming a more significant revenue source.\"\n",
    "print(\"- To maintain growth, the company needs to continue both acquiring new customers and nurturing existing ones to increase their lifetime value.\"\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1734cd",
   "metadata": {},
   "source": [
    "## Page 16\n",
    "\n",
    "Interpretation of Monthly Revenue Trend:\n",
    "- Initially, the revenue is entirely driven by new customers, as expected in th\n",
    "e first month of data.\n",
    "- Over time, the revenue from existing customers grows steadily, indicating suc\n",
    "cessful customer retention and repeat purchases.\n",
    "- Revenue from new customers fluctuates month-to-month but generally contribute\n",
    "s a significant portion of the total revenue.\n",
    "- The increasing contribution of existing customers to the overall revenue sugg\n",
    "ests that retention efforts are having a positive impact.\n",
    "- The balance between acquisition and retention seems to be shifting towards a\n",
    "healthier mix as the year progresses, with existing customers becoming a more s\n",
    "ignificant revenue source.\n",
    "- To maintain growth, the company needs to continue both acquiring new customer\n",
    "s and nurturing existing ones to increase their lifetime value.\n",
    "Business question 6\n",
    "Analyze the r elationship between coupon usage and r evenue generation and\n",
    "suggest optimized discount strategies.\n",
    "coupon_revenue =merged_df [merged_df ['Coupon_Status' ].isin(['Used','Clicked' ])][\n",
    "no_coupon_revenue =merged_df [merged_df ['Coupon_Status' ]=='Not Used' ]['Invoice_Value'\n",
    "coupon_avg_invoice_value =merged_df [merged_df ['Coupon_Status' ].isin(['Used','Clicked'\n",
    "no_coupon_avg_invoice_value =merged_df [merged_df ['Coupon_Status' ]=='Not Used'\n",
    "print(f\"Total Revenue with Coupon: {coupon_revenue :.2f}\")\n",
    "print(f\"Total Revenue without Coupon: {no_coupon_revenue :.2f}\")\n",
    "print(f\"Average Invoice Value with Coupon: {coupon_avg_invoice_value :.2f}\")\n",
    "print(f\"Average Invoice Value without Coupon: {no_coupon_avg_invoice_value :.2f}\n",
    "Total Revenue with Coupon: 3980519.82\n",
    "Total Revenue without Coupon: 733991.73\n",
    "Average Invoice Value with Coupon: 88.79\n",
    "Average Invoice Value without Coupon: 90.68\n",
    "print(\"\\nComparison of Revenue and Average Invoice Value:\" )\n",
    "print(f\"- Transactions with coupons generated significantly higher total revenue (\n",
    "print(f\"- The average invoice value for transactions with coupons ( {coupon_avg_invoice_value\n",
    "print(\"\\nOptimized Discount Strategies:\" )\n",
    "print(\"- While coupons drive higher total revenue, they slightly reduce the average transaction value. To maximize revenue while maintaining profitability, consider strategic coupon usage:\"\n",
    "print(\"  - **Targeted Coupons:** Offer higher discount percentages or specific product category coupons to high-value customers or for products with higher profit margins.\"\n",
    "print(\"  - **Conditional Discounts:** Implement minimum purchase value thresholds for coupon usage to encourage customers to spend more.\"\n",
    "print(\"  - **Limited-Time Offers:** Create a sense of urgency with time-limited coupon campaigns to boost sales within a specific period.\"\n",
    "print(\"  - **Analyze Coupon Effectiveness by Category:** Further analyze which product categories benefit most from discounts and tailor coupon strategies accordingly.\"\n",
    "print(\"  - **Consider Profitability:** While revenue is important, always consider the impact of discounts on overall profitability. Analyze the cost of discounts against the generated revenue.\"\n",
    "print(\"  - **A/B Testing:** Experiment with different discount percentages and coupon types to see which ones yield the best results in terms of both revenue and profitability.\"\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c7964",
   "metadata": {},
   "source": [
    "## Page 17\n",
    "\n",
    "Comparison of Revenue and Average Invoice Value:\n",
    "- Transactions with coupons generated significantly higher total revenue (39805\n",
    "19.82) compared to transactions without coupons (733991.73).\n",
    "- The average invoice value for transactions with coupons (88.79) is slightly l\n",
    "ower than transactions without coupons (90.68).\n",
    "Optimized Discount Strategies:\n",
    "- While coupons drive higher total revenue, they slightly reduce the average tr\n",
    "ansaction value. To maximize revenue while maintaining profitability, consider\n",
    "strategic coupon usage:\n",
    "- **Targeted Coupons:** Offer higher discount percentages or specific product\n",
    "category coupons to high-value customers or for products with higher profit mar\n",
    "gins.\n",
    "- **Conditional Discounts:** Implement minimum purchase value thresholds for\n",
    "coupon usage to encourage customers to spend more.\n",
    "- **Limited-Time Offers:** Create a sense of urgency with time-limited coupon\n",
    "campaigns to boost sales within a specific period.\n",
    "- **Analyze Coupon Effectiveness by Category:** Further analyze which product\n",
    "categories benefit most from discounts and tailor coupon strategies accordingl\n",
    "y.\n",
    "- **Consider Profitability:** While revenue is important, always consider the\n",
    "impact of discounts on overall profitability. Analyze the cost of discounts aga\n",
    "inst the generated revenue.\n",
    "- **A/B Testing:** Experiment with different discount percentages and coupon\n",
    "types to see which ones yield the best results in terms of both revenue and pro\n",
    "fitability.\n",
    "Business question 7\n",
    "Identif y the top-perfor ming pr oducts and analyze the factors driving their success.\n",
    "How can this insight infor m inventory management and pr omotional strategies?\n",
    "product_performance =merged_df .groupby('Product_SKU' ).agg(\n",
    "Total_Revenue =('Invoice_Value' ,'sum'),\n",
    "Total_Quantity_Sold =('Quantity' ,'sum')\n",
    ").reset_index ()\n",
    "top_revenue_products =product_performance .sort_values (by='Total_Revenue' ,ascending\n",
    "top_quantity_products =product_performance .sort_values (by='Total_Quantity_Sold'\n",
    "print(\"Top 20 Products by Revenue:\" )\n",
    "display(top_revenue_products )\n",
    "print(\"\\nTop 20 Products by Quantity Sold:\" )\n",
    "display(top_quantity_products )\n",
    "Top 20 Products by Revenue:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379da15b",
   "metadata": {},
   "source": [
    "## Page 18\n",
    "\n",
    "Product_SK UTotal_Revenue Total_Quantity_Sold\n",
    "981 GGOENEBJ079499 633770.95210 4570\n",
    "983 GGOENEBQ078999 578973.57080 5206\n",
    "976 GGOENEBB078899 490219.16470 4402\n",
    "984 GGOENEBQ079099 198503.43240 2683\n",
    "985 GGOENEBQ079199 195517.18510 2670\n",
    "989 GGOENEBQ084699 185669.71800 1368\n",
    "994 GGOENEBQ092299 148256.80190 510\n",
    "990 GGOENEBQ086499 130498.55220 771\n",
    "992 GGOENEBQ086799 94622.19640 1091\n",
    "980 GGOENEBD084799 64051.32960 472\n",
    "995 GGOENEBQ092399 55449.45000 225\n",
    "892 GGOEGDHQ015399 37678.89416 1567\n",
    "854 GGOEGBMJ013399 36464.53284 7321\n",
    "969 GGOEGOLC014299 36152.19760 6496\n",
    "952 GGOEGOCB017499 35374.34180 3741\n",
    "882 GGOEGDHC074099 34233.80698 1857\n",
    "880 GGOEGDHC018299 33244.58676 9728\n",
    "961 GGOEGOCL077699 29827.10230 2209\n",
    "865 GGOEGBRJ037399 26119.37394 398\n",
    "982 GGOENEBJ081899 25062.32360 136\n",
    "Top 20 Products by Quantity Sold:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95605a",
   "metadata": {},
   "source": [
    "## Page 19\n",
    "\n",
    "Product_SK UTotal_Revenue Total_Quantity_Sold\n",
    "914 GGOEGGO A017399 18226.27320 16234\n",
    "880 GGOEGDHC018299 33244.58676 9728\n",
    "854 GGOEGBMJ013399 36464.53284 7321\n",
    "969 GGOEGOLC014299 36152.19760 6496\n",
    "904 GGOEGFKQ020399 22729.54980 5847\n",
    "907 GGOEGF SR022099 13870.08992 5549\n",
    "983 GGOENEBQ078999 578973.57080 5206\n",
    "908 GGOEGFYQ016599 10393.89898 5098\n",
    "942 GGOEGO AQ012899 13995.84400 4861\n",
    "981 GGOENEBJ079499 633770.95210 4570\n",
    "894 GGOEGDHR018499 16009.69240 4554\n",
    "976 GGOENEBB078899 490219.16470 4402\n",
    "945 GGOEGO AQ020099 14376.15870 4394\n",
    "952 GGOEGOCB017499 35374.34180 3741\n",
    "940 GGOEGO AC021799 5063.53340 3576\n",
    "916 GGOEGHGH019699 16660.61078 3463\n",
    "963 GGOEGOCR017899 13379.65520 3263\n",
    "195 GGOEAKDH019899 15688.88142 3066\n",
    "847 GGOEGBJC019999 15133.86880 2956\n",
    "915 GGOEGHGC019799 15554.14826 2922\n",
    "top_product_skus =pd.concat([top_revenue_products ['Product_SKU' ],top_quantity_products\n",
    "top_products_info =online_sales_df [online_sales_df ['Product_SKU' ].isin(top_product_skus\n",
    "print(\"\\nInformation about Top Performing Products:\" )\n",
    "display(top_products_info )\n",
    "print(\"\\nStrategies based on Top Performing Products Analysis:\" )\n",
    "print(\"- **Inventory Management:** Ensure sufficient stock levels for these top-performing products to meet consistent demand and prevent stockouts, which can lead to lost sales and customer dissatisfaction.\"\n",
    "print(\"- **Promotional Strategies:** Feature these high-performing products prominently in marketing campaigns, advertisements, and website promotions to capitalize on their popularity and drive further sales.\"\n",
    "print(\"- **Bundling and Cross-selling:** Consider creating bundles that include top products with complementary items to increase average order value.\"\n",
    "print(\"- **Analyze Success Factors:** Investigate the specific characteristics that make these products successful. Is it their price point, quality, marketing, or a specific feature? Use these insights to inform product development and marketing for other products.\"\n",
    "print(\"- **Geographic and Seasonal Analysis:** Analyze if the performance of these top products varies by location or season. Tailor inventory and promotional strategies accordingly.\"\n",
    "print(\"- **Customer Reviews and Feedback:** Monitor customer reviews and feedback for these top products to understand what customers like and dislike, and use this information to improve product offerings and marketing messages.\"\n",
    "Information about Top Performing Products:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3641666",
   "metadata": {},
   "source": [
    "## Page 20\n",
    "\n",
    "Product_SK U Product_Description Product_Category\n",
    "0GGOENEBJ079499Nest L earning Ther mostat 3r d Gen-\n",
    "USA - Stainle...Nest-US A\n",
    "2GGOEGFKQ020399Google Laptop and Cell Phone\n",
    "Stick ersOffice\n",
    "5GGOEGBMJ013399 Sport Bag Bags\n",
    "6GGOEGDHC018299 Google 22 oz W ater Bottle Drinkwar e\n",
    "9GGOEGGO A017399 Maze P en Office\n",
    "13 GGOENEBQ078999Nest Cam Outdoor Security Camera -\n",
    "USANest-US A\n",
    "14 GGOENEBQ079199Nest P rotect Smok e + CO White W ired\n",
    "Alarm-US ANest-US A\n",
    "20GGOEGO AQ012899 Ballpoint LED Light P en Office\n",
    "24 GGOENEBB078899Nest Cam Indoor Security Camera -\n",
    "USANest-US A\n",
    "32GGOEGDHC074099Google 17oz Stainless Steel Sport\n",
    "BottleDrinkwar e\n",
    "43 GGOEGF SR022099 Google Kick Ball Lifestyle\n",
    "51 GGOENEBQ079099Nest P rotect Smok e + CO White\n",
    "Battery Alar m-US ANest-US A\n",
    "59 GGOEGBJC019999 Collapsible Shopping Bag Bags\n",
    "90 GGOEAKDH019899 Windup Andr oid Lifestyle\n",
    "92 GGOENEBJ081899Nest L earning Ther mostat 3r d Gen -\n",
    "CA - Stainl...Nest-Canada\n",
    "126 GGOEGOCR017899 Recycled P aper Jour nal Set Office\n",
    "154 GGOEGDHQ015399 26 oz Double W all Insulated Bottle Drinkwar e\n",
    "195 GGOEGHGH019699 Google Sunglasses Lifestyle\n",
    "232 GGOEGHGC019799 Google Sunglasses Lifestyle\n",
    "265 GGOEGFYQ016599 Foam Can and Bottle Cooler Drinkwar e\n",
    "278 GGOEGOCB017499 Leather ette Jour nal Office\n",
    "307 GGOEGDHR018499 Google 22 oz W ater Bottle Drinkwar e\n",
    "420 GGOEGOLC014299 Google Metallic Notebook Set Office\n",
    "571 GGOEGO AQ020099 Four Color R etractable P en Office\n",
    "665 GGOEGOCL077699 Google Har d Cover Jour nalNotebooks &\n",
    "Journals\n",
    "693 GGOEGO AC021799 Ballpoint P en Blue Office"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3de77",
   "metadata": {},
   "source": [
    "## Page 21\n",
    "\n",
    "Product_SK U Product_Description Product_Category\n",
    "735 GGOEGBRJ037399 Google R ucksack Bags\n",
    "12134 GGOENEBD084799Nest L earning Ther mostat 3r d Gen-\n",
    "USA - CopperNest-US A\n",
    "12142 GGOENEBQ084699Nest L earning Ther mostat 3r d Gen-\n",
    "USA - WhiteNest-US A\n",
    "31870 GGOENEBQ086499 Nest Cam IQ - US A Nest\n",
    "39002 GGOENEBQ092299Nest Secur e Alar m System Starter\n",
    "Pack - US ANest\n",
    "39027 GGOENEBQ092399 Nest Cam IQ Outdoor - US A (Preorder) Nest\n",
    "39492 GGOENEBQ086799 Nest Ther mostat E - US A Nest\n",
    "Strategies based on Top Performing Products Analysis:\n",
    "- **Inventory Management:** Ensure sufficient stock levels for these top-perfor\n",
    "ming products to meet consistent demand and prevent stockouts, which can lead t\n",
    "o lost sales and customer dissatisfaction.\n",
    "- **Promotional Strategies:** Feature these high-performing products prominentl\n",
    "y in marketing campaigns, advertisements, and website promotions to capitalize\n",
    "on their popularity and drive further sales.\n",
    "- **Bundling and Cross-selling:** Consider creating bundles that include top pr\n",
    "oducts with complementary items to increase average order value.\n",
    "- **Analyze Success Factors:** Investigate the specific characteristics that ma\n",
    "ke these products successful. Is it their price point, quality, marketing, or a\n",
    "specific feature? Use these insights to inform product development and marketin\n",
    "g for other products.\n",
    "- **Geographic and Seasonal Analysis:** Analyze if the performance of these top\n",
    "products varies by location or season. Tailor inventory and promotional strateg\n",
    "ies accordingly.\n",
    "- **Customer Reviews and Feedback:** Monitor customer reviews and feedback for\n",
    "these top products to understand what customers like and dislike, and use this\n",
    "information to improve product offerings and marketing messages.\n",
    "Business question 8\n",
    "Analyze the r elationship between monthly mark eting spend and r evenue, identif y\n",
    "months with dispr oportionately high or low r eturns, and suggest adjustments to\n",
    "improve ROI.\n",
    "marketing_spend_df ['Date']=pd.to_datetime (marketing_spend_df ['Date'])\n",
    "marketing_spend_df ['Month']=marketing_spend_df ['Date'].dt.to_period ('M')\n",
    "monthly_marketing_spend =marketing_spend_df .groupby('Month').agg(\n",
    "Total_Offline_Spend =('Offline_Spend' ,'sum'),\n",
    "Total_Online_Spend =('Online_Spend' ,'sum')\n",
    ").reset_index ()\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ce18a",
   "metadata": {},
   "source": [
    "## Page 22\n",
    "\n",
    "monthly_marketing_spend ['Total_Marketing_Spend' ]=monthly_marketing_spend ['Total_Offline_Spend'\n",
    "monthly_revenue_total =merged_df .groupby('Transaction_Month' )['Invoice_Value' ]\n",
    "monthly_revenue_total .rename(columns={'Transaction_Month' :'Month','Invoice_Value'\n",
    "# Convert 'Month' column in monthly_revenue_total to Period objects to match monthly_marketing_spend\n",
    "monthly_revenue_total ['Month']=monthly_revenue_total ['Month'].astype('period[M]'\n",
    "merged_marketing_revenue =pd.merge(monthly_marketing_spend ,monthly_revenue_total\n",
    "# Calculate ROI, handling potential division by zero\n",
    "merged_marketing_revenue ['ROI']=merged_marketing_revenue .apply(\n",
    "lambda row:((row['Total_Revenue' ]-row['Total_Marketing_Spend' ])/row['Total_Marketing_Spend'\n",
    "axis=1\n",
    ")\n",
    "print(\"Monthly Marketing Spend, Revenue, and ROI:\" )\n",
    "display(merged_marketing_revenue )\n",
    "# Identify months with disproportionately high or low ROI\n",
    "mean_roi =merged_marketing_revenue ['ROI'].mean()\n",
    "std_roi =merged_marketing_revenue ['ROI'].std()\n",
    "high_roi_months =merged_marketing_revenue [merged_marketing_revenue ['ROI']>mean_roi\n",
    "low_roi_months =merged_marketing_revenue [merged_marketing_revenue ['ROI']<mean_roi\n",
    "print(f\"\\nAverage ROI: {mean_roi :.2f}%\")\n",
    "print(f\"Standard Deviation of ROI: {std_roi:.2f}%\")\n",
    "print(\"\\nMonths with Disproportionately High ROI:\" )\n",
    "display(high_roi_months )\n",
    "print(\"\\nMonths with Disproportionately Low ROI:\" )\n",
    "display(low_roi_months )\n",
    "print(\"\\nSuggested Adjustments to Marketing Strategies:\" )\n",
    "print(\"- **For High ROI Months:** Analyze the specific marketing activities, campaigns, and channels that were most effective during these months. Replicate or scale up these successful strategies in other periods. Understand the customer behavior and market conditions that contributed to the high returns.\"\n",
    "print(\"- **For Low ROI Months:** Investigate the marketing spend and activities during these months. Identify underperforming campaigns or channels and consider reallocating budget to more effective areas. Analyze customer engagement and conversion rates during these periods to pinpoint issues. Consider testing new marketing approaches or focusing on different customer segments.\"\n",
    "print(\"- **Continuous Monitoring:** Regularly track and analyze marketing spend and revenue to identify trends and adjust strategies promptly.\"\n",
    "print(\"- **Attribute ROI to Specific Campaigns:** If possible, try to attribute marketing spend and revenue to specific campaigns to get a more granular understanding of what is working and what is not.\"\n",
    "Monthly Marketing Spend, Revenue, and ROI:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784e197",
   "metadata": {},
   "source": [
    "## Page 23\n",
    "\n",
    "Month Total_Offline_Spend Total_Online_Spend Total_Mark eting_Spend Total_Revenue\n",
    "02019-01 96600 58328.95 154928.95 463883.05705\n",
    "12019-02 81300 55807.92 137107.92 327896.56020\n",
    "22019-03 73500 48750.09 122250.09 336805.20383\n",
    "32019-04 96000 61026.83 157026.83 447999.19523\n",
    "42019-05 65500 52759.64 118259.64 318556.30056\n",
    "52019-06 80500 53818.14 134318.14 289830.32931\n",
    "62019-07 67500 52717.85 120217.85 423982.34361\n",
    "72019-08 85500 57404.15 142904.15 418160.56704\n",
    "82019-09 83000 52514.54 135514.54 321128.35638\n",
    "92019-10 93500 57724.65 151224.65 450837.46255\n",
    "102019-11 93000 68144.96 161144.96 475902.15336\n",
    "112019-12 122000 76648.75 198648.75 439530.03015\n",
    "Average ROI: 173.46%\n",
    "Standard Deviation of ROI: 39.46%\n",
    "Months with Disproportionately High ROI:\n",
    "Month Total_Offline_Spend Total_Online_Spend Total_Mark eting_Spend Total_Revenue\n",
    "62019-07 67500 52717.85 120217.85 423982.34361\n",
    "Months with Disproportionately Low ROI:\n",
    "Month Total_Offline_Spend Total_Online_Spend Total_Mark eting_Spend Total_Revenue\n",
    "52019-06 80500 53818.14 134318.14 289830.32931\n",
    "112019-12 122000 76648.75 198648.75 439530.03015\n",
    "Suggested Adjustments to Marketing Strategies:\n",
    "- **For High ROI Months:** Analyze the specific marketing activities, campaign\n",
    "s, and channels that were most effective during these months. Replicate or scal\n",
    "e up these successful strategies in other periods. Understand the customer beha\n",
    "vior and market conditions that contributed to the high returns.\n",
    "- **For Low ROI Months:** Investigate the marketing spend and activities during\n",
    "these months. Identify underperforming campaigns or channels and consider reall\n",
    "ocating budget to more effective areas. Analyze customer engagement and convers\n",
    "ion rates during these periods to pinpoint issues. Consider testing new marketi\n",
    "ng approaches or focusing on different customer segments.\n",
    "- **Continuous Monitoring:** Regularly track and analyze marketing spend and re\n",
    "venue to identify trends and adjust strategies promptly.\n",
    "- **Attribute ROI to Specific Campaigns:** If possible, try to attribute market\n",
    "ing spend and revenue to specific campaigns to get a more granular understandin\n",
    "g of what is working and what is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b5e1e",
   "metadata": {},
   "source": [
    "## Page 24\n",
    "\n",
    "Business question 9\n",
    "Evaluate the effectiveness of mark eting campaigns by comparing mark eting spend\n",
    "to revenue and identif y opportunities for r esour ce reallocation.\n",
    "import matplotlib.pyplot asplt\n",
    "merged_marketing_revenue ['Marketing_Spend_Percentage' ]=(merged_marketing_revenue\n",
    "print(\"Monthly Marketing Spend as a Percentage of Revenue:\" )\n",
    "display(merged_marketing_revenue [['Month','Total_Marketing_Spend' ,'Total_Revenue'\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(merged_marketing_revenue ['Month'].astype(str),merged_marketing_revenue\n",
    "plt.title('Monthly Marketing Spend as Percentage of Revenue' )\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Marketing Spend Percentage (%)' )\n",
    "plt.xticks(rotation =45)\n",
    "plt.grid(True )\n",
    "plt.tight_layout ()\n",
    "plt.show()\n",
    "Monthly Marketing Spend as a Percentage of Revenue:\n",
    "Month Total_Mark eting_Spend Total_Revenue Mark eting_Spend_Percentage\n",
    "02019-01 154928.95 463883.05705 33.398277\n",
    "12019-02 137107.92 327896.56020 41.814382\n",
    "22019-03 122250.09 336805.20383 36.296972\n",
    "32019-04 157026.83 447999.19523 35.050695\n",
    "42019-05 118259.64 318556.30056 37.123623\n",
    "52019-06 134318.14 289830.32931 46.343714\n",
    "62019-07 120217.85 423982.34361 28.354447\n",
    "72019-08 142904.15 418160.56704 34.174468\n",
    "82019-09 135514.54 321128.35638 42.199494\n",
    "92019-10 151224.65 450837.46255 33.543053\n",
    "102019-11 161144.96 475902.15336 33.860944\n",
    "112019-12 198648.75 439530.03015 45.195717\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868ce1c",
   "metadata": {},
   "source": [
    "## Page 25\n",
    "\n",
    "print(\"\\nAnalysis of Monthly Marketing Spend as Percentage of Revenue:\" )\n",
    "print(\"- The percentage of marketing spend relative to revenue fluctuates throughout the year.\"\n",
    "print(\"- Months with a lower percentage of marketing spend relative to revenue (e.g., July at 28.35%) might indicate more efficient marketing or higher revenue generated per marketing dollar.\"\n",
    "print(\"- Months with a higher percentage (e.g., June at 46.34 % and December at 45.19%) could suggest lower marketing efficiency or higher investment periods that didn't yield proportionally higher revenue.\"\n",
    "print(\"\\nOpportunities for Reallocating Marketing Resources:\" )\n",
    "print(\"- **Shift Resources from High Percentage Months:** Analyze the specific campaigns and channels used in months with a high marketing spend percentage (like June and December). If certain activities consistently show a lower return on investment, consider reducing or reallocating the budget from these to more effective areas.\"\n",
    "print(\"- **Invest More in Low Percentage Months:** Examine the strategies employed in months with a low marketing spend percentage (like July). These might represent successful approaches that could be scaled up or replicated in other months to potentially achieve higher revenue without a proportional increase in spend.\"\n",
    "print(\"- **Seasonal Adjustments:** Consider if the higher percentages in June and December are due to increased seasonal marketing efforts that did not translate into significantly higher revenue. Re-evaluate the effectiveness of seasonal campaigns and adjust spending or messaging accordingly.\"\n",
    "print(\"- **Channel Performance Analysis:** Break down marketing spend and revenue by channel (e.g., online vs. offline) within each month to identify which channels are most efficient and reallocate resources towards those with better performance.\"\n",
    "print(\"- **Experimentation:** Use the insights from high and low percentage months to inform A/B testing of different marketing messages, offers, and targeting strategies.\"\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a46994",
   "metadata": {},
   "source": [
    "## Page 26\n",
    "\n",
    "Analysis of Monthly Marketing Spend as Percentage of Revenue:\n",
    "- The percentage of marketing spend relative to revenue fluctuates throughout t\n",
    "he year.\n",
    "- Months with a lower percentage of marketing spend relative to revenue (e.g.,\n",
    "July at 28.35%) might indicate more efficient marketing or higher revenue gener\n",
    "ated per marketing dollar.\n",
    "- Months with a higher percentage (e.g., June at 46.34% and December at 45.19%)\n",
    "could suggest lower marketing efficiency or higher investment periods that did\n",
    "n't yield proportionally higher revenue.\n",
    "Opportunities for Reallocating Marketing Resources:\n",
    "- **Shift Resources from High Percentage Months:** Analyze the specific campaig\n",
    "ns and channels used in months with a high marketing spend percentage (like Jun\n",
    "e and December). If certain activities consistently show a lower return on inve\n",
    "stment, consider reducing or reallocating the budget from these to more effecti\n",
    "ve areas.\n",
    "- **Invest More in Low Percentage Months:** Examine the strategies employed in\n",
    "months with a low marketing spend percentage (like July). These might represent\n",
    "successful approaches that could be scaled up or replicated in other months to\n",
    "potentially achieve higher revenue without a proportional increase in spend.\n",
    "- **Seasonal Adjustments:** Consider if the higher percentages in June and Dece\n",
    "mber are due to increased seasonal marketing efforts that did not translate int\n",
    "o significantly higher revenue. Re-evaluate the effectiveness of seasonal campa\n",
    "igns and adjust spending or messaging accordingly.\n",
    "- **Channel Performance Analysis:** Break down marketing spend and revenue by c\n",
    "hannel (e.g., online vs. offline) within each month to identify which channels\n",
    "are most efficient and reallocate resources towards those with better performan\n",
    "ce.\n",
    "- **Experimentation:** Use the insights from high and low percentage months to\n",
    "inform A/B testing of different marketing messages, offers, and targeting strat\n",
    "egies.\n",
    "Business question 10\n",
    "Segment customers into P remium, Gold, Silver, and Standar d using RFM\n",
    "techniques and suggest tar geted strategies for each segment.\n",
    "from datetime import datetime\n",
    "# Convert Transaction_Date_transaction to datetime objects\n",
    "merged_df ['Transaction_Date_transaction' ]=pd.to_datetime (merged_df ['Transaction_Date_transaction'\n",
    "# Determine the most recent transaction date in the entire dataset\n",
    "latest_date =merged_df ['Transaction_Date_transaction' ].max()\n",
    "# Calculate Recency, Frequency, and Monetary values\n",
    "rfm_df=merged_df .groupby('CustomerID' ).agg(\n",
    "Recency=('Transaction_Date_transaction' ,lambda date:(latest_date -date.max\n",
    "Frequency =('Transaction_ID' ,'nunique' ),\n",
    "Monetary =('Invoice_Value' ,'sum')\n",
    ").reset_index ()\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d405848",
   "metadata": {},
   "source": [
    "## Page 27\n",
    "\n",
    "display(rfm_df.head())\n",
    "CustomerID Recency Frequency Monetary\n",
    "0 12346 107 1 174.98174\n",
    "1 12347 59 3112090.29580\n",
    "2 12348 73 81501.92620\n",
    "3 12350 17 11 1183.71394\n",
    "4 12356 107 13 1753.45086\n",
    "# Create RFM scores using quartiles\n",
    "rfm_df['R_Score' ]=pd.qcut(rfm_df['Recency' ],4,labels=[4,3,2,1])\n",
    "rfm_df['F_Score' ]=pd.qcut(rfm_df['Frequency' ],4,labels=[1,2,3,4])\n",
    "rfm_df['M_Score' ]=pd.qcut(rfm_df['Monetary' ],4,labels=[1,2,3,4])\n",
    "# Combine RFM scores to create a segment\n",
    "rfm_df['RFM_Segment' ]=rfm_df['R_Score' ].astype(str)+rfm_df['F_Score' ].astype\n",
    "# Define customer segments based on RFM score\n",
    "# This is a common way to define segments, but can be adjusted based on business needs\n",
    "def segment_customers (rfm_segment ):\n",
    "ifrfm_segment in['444','443','434','344','433','343','334']:\n",
    "return 'Premium'\n",
    "elif rfm_segment in['424','423','414','413','324','323','314','313'\n",
    "return 'Gold'\n",
    "elif rfm_segment in['422','421','412','411','322','321','312','311'\n",
    "return 'Silver'\n",
    "else :\n",
    "return 'Standard'\n",
    "rfm_df['Customer_Segment' ]=rfm_df['RFM_Segment' ].apply(segment_customers )\n",
    "display(rfm_df.head())\n",
    "CustomerID Recency Frequency Monetary R_Score F_Score M_Score RFM_Segment\n",
    "0 12346 107 1 174.98174 3 1 1\n",
    "1 12347 59 3112090.29580 3 4 4\n",
    "2 12348 73 81501.92620 3 2 2\n",
    "3 12350 17 11 1183.71394 4 2 2\n",
    "4 12356 107 13 1753.45086 3 3 2\n",
    "# Analyze the characteristics of each customer segment\n",
    "segment_characteristics =rfm_df.groupby('Customer_Segment' )[['Recency' ,'Frequency'\n",
    "print(\"Characteristics of Each Customer Segment (Average RFM Values):\" )\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70772a0",
   "metadata": {},
   "source": [
    "## Page 28\n",
    "\n",
    "display(segment_characteristics .sort_values (by='Monetary' ,ascending =False ))\n",
    "Characteristics of Each Customer Segment (Average RFM Values):\n",
    "Customer_Segment Recency Frequency Monetary\n",
    "1 Premium 48.169014 40.509859 7495.142187\n",
    "0 Gold 144.637500 23.168750 4064.239823\n",
    "2 Silver 136.022444 11.812968 2099.312624\n",
    "3 Standar d212.018116 6.894928 1017.451719\n",
    "print(\"\\nTargeted Strategies for Each Customer Segment:\" )\n",
    "print(\"\\n**Premium Segment (Low Recency, High Frequency, High Monetary):**\" )\n",
    "print(\"- These are the most valuable customers. Focus on retention and loyalty programs.\"\n",
    "print(\"- Offer exclusive access to new products or sales.\" )\n",
    "print(\"- Provide personalized recommendations and VIP customer service.\" )\n",
    "print(\"- Consider a tiered loyalty program with increasing benefits.\" )\n",
    "print(\"- Encourage referrals to acquire similar high-value customers.\" )\n",
    "print(\"\\n**Gold Segment (Mid-High Recency, Mid-High Frequency, Mid-High Monetary):**\"\n",
    "print(\"- These customers are valuable but might need nurturing to become Premium.\"\n",
    "print(\"- Implement targeted email campaigns with personalized offers based on their purchase history.\"\n",
    "print(\"- Encourage increased purchase frequency through subscription options or loyalty points.\"\n",
    "print(\"- Offer early access to sales or special events.\" )\n",
    "print(\"- Provide excellent customer service to maintain satisfaction.\" )\n",
    "print(\"\\n**Silver Segment (Mid Recency, Mid Frequency, Mid Monetary):**\" )\n",
    "print(\"- These customers have made some purchases but may not be as frequent or high-spending.\"\n",
    "print(\"- Use re-engagement campaigns to encourage repeat purchases if their recency is increasing.\"\n",
    "print(\"- Offer targeted discounts or promotions to incentivize them to buy more frequently or higher-value items.\"\n",
    "print(\"- Recommend related products based on their past purchases.\" )\n",
    "print(\"- Collect feedback to understand their needs and preferences.\" )\n",
    "print(\"\\n**Standard Segment (High Recency, Low Frequency, Low Monetary):**\" )\n",
    "print(\"- These customers are either new or infrequent buyers with low spending.\"\n",
    "print(\"- Focus on activation and increasing their first or next purchase.\" )\n",
    "print(\"- Offer welcome discounts or incentives for their first purchase if they are new.\"\n",
    "print(\"- Implement win-back campaigns for inactive customers.\" )\n",
    "print(\"- Provide clear product information and easy navigation on your website.\"\n",
    "print(\"- Consider lower-priced entry-point products to encourage initial engagement.\"\n",
    "In[1]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705deaf6",
   "metadata": {},
   "source": [
    "## Page 29\n",
    "\n",
    "Targeted Strategies for Each Customer Segment:\n",
    "**Premium Segment (Low Recency, High Frequency, High Monetary):**\n",
    "- These are the most valuable customers. Focus on retention and loyalty program\n",
    "s.\n",
    "- Offer exclusive access to new products or sales.\n",
    "- Provide personalized recommendations and VIP customer service.\n",
    "- Consider a tiered loyalty program with increasing benefits.\n",
    "- Encourage referrals to acquire similar high-value customers.\n",
    "**Gold Segment (Mid-High Recency, Mid-High Frequency, Mid-High Monetary):**\n",
    "- These customers are valuable but might need nurturing to become Premium.\n",
    "- Implement targeted email campaigns with personalized offers based on their pu\n",
    "rchase history.\n",
    "- Encourage increased purchase frequency through subscription options or loyalt\n",
    "y points.\n",
    "- Offer early access to sales or special events.\n",
    "- Provide excellent customer service to maintain satisfaction.\n",
    "**Silver Segment (Mid Recency, Mid Frequency, Mid Monetary):**\n",
    "- These customers have made some purchases but may not be as frequent or high-s\n",
    "pending.\n",
    "- Use re-engagement campaigns to encourage repeat purchases if their recency is\n",
    "increasing.\n",
    "- Offer targeted discounts or promotions to incentivize them to buy more freque\n",
    "ntly or higher-value items.\n",
    "- Recommend related products based on their past purchases.\n",
    "- Collect feedback to understand their needs and preferences.\n",
    "**Standard Segment (High Recency, Low Frequency, Low Monetary):**\n",
    "- These customers are either new or infrequent buyers with low spending.\n",
    "- Focus on activation and increasing their first or next purchase.\n",
    "- Offer welcome discounts or incentives for their first purchase if they are ne\n",
    "w.\n",
    "- Implement win-back campaigns for inactive customers.\n",
    "- Provide clear product information and easy navigation on your website.\n",
    "- Consider lower-priced entry-point products to encourage initial engagement.\n",
    "Business question 11\n",
    "Analyze the r evenue contribution of each customer segment and suggest how to\n",
    "focus efforts on high-value segments while nurturing lower -value ones.\n",
    "customer_revenue =merged_df .groupby('CustomerID' )['Invoice_Value' ].sum().reset_index\n",
    "customer_revenue .rename(columns={'Invoice_Value' :'Total_Customer_Revenue' },inplace\n",
    "rfm_revenue_df =pd.merge(rfm_df,customer_revenue ,on='CustomerID' ,how='left'\n",
    "segment_revenue =rfm_revenue_df .groupby('Customer_Segment' )['Total_Customer_Revenue'\n",
    "print(\"Total Revenue Generated by Each Customer Segment:\" )\n",
    "display(segment_revenue )\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3b29a",
   "metadata": {},
   "source": [
    "## Page 30\n",
    "\n",
    "Total Revenue Generated by Each Customer Segment:\n",
    "Customer_Segment Total_Customer_Revenue\n",
    "0 Gold 6.502784e+05\n",
    "1 Premium 2.660775e+06\n",
    "2 Silver 8.418244e+05\n",
    "3 Standar d 5.616333e+05\n",
    "total_revenue_all_segments =segment_revenue ['Total_Customer_Revenue' ].sum()\n",
    "segment_revenue ['Revenue_Percentage' ]=(segment_revenue ['Total_Customer_Revenue'\n",
    "print(\"\\nPercentage of Total Revenue Contributed by Each Customer Segment:\" )\n",
    "display(segment_revenue .sort_values (by='Revenue_Percentage' ,ascending =False ))\n",
    "print(\"\\nStrategies to Focus Efforts on High-Value Segments and Nurture Lower-Value Segments:\"\n",
    "print(\"- **Focus on Premium and Gold (High-Value Segments):** These segments contribute the largest share of revenue. Prioritize retaining these customers through loyalty programs, exclusive offers, personalized experiences, and exceptional customer service. Invest in understanding their needs and preferences to continuously provide value and encourage repeat high-value purchases.\"\n",
    "print(\"- **Nurture Silver and Standard (Lower-Value Segments):** While contributing less individually, these segments represent potential for growth. Implement targeted strategies to move them up the customer value ladder. For Silver customers, focus on increasing purchase frequency and average order value through targeted promotions and product recommendations. For Standard customers (especially new ones), focus on encouraging their second purchase and building initial loyalty. Implement win-back campaigns for inactive customers in these segments.\"\n",
    "print(\"- **Allocate Marketing Budget Proportionally:** Consider allocating a larger portion of the marketing budget to campaigns targeting high-value segments, as the return on investment is likely to be higher. However, do not neglect lower-value segments; allocate resources for targeted campaigns aimed at increasing their value.\"\n",
    "print(\"- **Personalize Communication and Offers:** Tailor marketing messages, promotions, and product recommendations to the specific needs and behaviors of each segment. High-value customers might respond better to exclusive access and loyalty rewards, while lower-value segments might be motivated by discounts and introductory offers.\"\n",
    "print(\"- **Analyze Segment Migration:** Track how customers move between segments over time. This can provide insights into the effectiveness of nurturing strategies for lower-value segments and identify potential issues causing high-value customers to downgrade.\"\n",
    "Percentage of Total Revenue Contributed by Each Customer Segment:\n",
    "Customer_Segment Total_Customer_Revenue Revenue_Percentage\n",
    "1 Premium 2.660775e+06 56.437988\n",
    "2 Silver 8.418244e+05 17.856025\n",
    "0 Gold 6.502784e+05 13.793123\n",
    "3 Standar d 5.616333e+05 11.912864\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769a06a",
   "metadata": {},
   "source": [
    "## Page 31\n",
    "\n",
    "Strategies to Focus Efforts on High-Value Segments and Nurture Lower-Value Segm\n",
    "ents:\n",
    "- **Focus on Premium and Gold (High-Value Segments):** These segments contribut\n",
    "e the largest share of revenue. Prioritize retaining these customers through lo\n",
    "yalty programs, exclusive offers, personalized experiences, and exceptional cus\n",
    "tomer service. Invest in understanding their needs and preferences to continuou\n",
    "sly provide value and encourage repeat high-value purchases.\n",
    "- **Nurture Silver and Standard (Lower-Value Segments):** While contributing le\n",
    "ss individually, these segments represent potential for growth. Implement targe\n",
    "ted strategies to move them up the customer value ladder. For Silver customers,\n",
    "focus on increasing purchase frequency and average order value through targeted\n",
    "promotions and product recommendations. For Standard customers (especially new\n",
    "ones), focus on encouraging their second purchase and building initial loyalty.\n",
    "Implement win-back campaigns for inactive customers in these segments.\n",
    "- **Allocate Marketing Budget Proportionally:** Consider allocating a larger po\n",
    "rtion of the marketing budget to campaigns targeting high-value segments, as th\n",
    "e return on investment is likely to be higher. However, do not neglect lower-va\n",
    "lue segments; allocate resources for targeted campaigns aimed at increasing the\n",
    "ir value.\n",
    "- **Personalize Communication and Offers:** Tailor marketing messages, promotio\n",
    "ns, and product recommendations to the specific needs and behaviors of each seg\n",
    "ment. High-value customers might respond better to exclusive access and loyalty\n",
    "rewards, while lower-value segments might be motivated by discounts and introdu\n",
    "ctory offers.\n",
    "- **Analyze Segment Migration:** Track how customers move between segments over\n",
    "time. This can provide insights into the effectiveness of nurturing strategies\n",
    "for lower-value segments and identify potential issues causing high-value custo\n",
    "mers to downgrade.\n",
    "Business question 12\n",
    "Group customers by their month of first pur chase, analyze r etention rates over\n",
    "time, identif y cohorts with highest/lowest r etention, and suggest strategies for\n",
    "weak er cohorts.\n",
    "online_sales_df ['Transaction_Date' ]=pd.to_datetime (online_sales_df ['Transaction_Date'\n",
    "first_purchase_df =online_sales_df .groupby('CustomerID' )['Transaction_Date' ].min\n",
    "first_purchase_df ['Acquisition_Month' ]=first_purchase_df ['Transaction_Date' ].\n",
    "display(first_purchase_df .head())\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd891c6",
   "metadata": {},
   "source": [
    "## Page 32\n",
    "\n",
    "CustomerID Transaction_Date Acquisition_Month\n",
    "0 12346 2019-09-15 2019-09\n",
    "1 12347 2019-03-24 2019-03\n",
    "2 12348 2019-06-22 2019-06\n",
    "3 12350 2019-12-14 2019-12\n",
    "4 12356 2019-09-15 2019-09\n",
    "online_sales_df ['Invoice_Month' ]=online_sales_df ['Transaction_Date' ].dt.to_period\n",
    "customer_monthly_purchase =online_sales_df .groupby('CustomerID' )['Invoice_Month'\n",
    "display(customer_monthly_purchase .head())\n",
    "CustomerID Purchase_Months\n",
    "0 12346 [2019-09]\n",
    "1 12347 [2019-03, 2019-11]\n",
    "2 12348 [2019-06, 2019-10]\n",
    "3 12350 [2019-12]\n",
    "4 12356 [2019-09]\n",
    "cohorts =first_purchase_df .set_index ('CustomerID' )['Acquisition_Month' ]\n",
    "all_months =sorted(online_sales_df ['Invoice_Month' ].unique())\n",
    "retention_matrix =pd.DataFrame (index=cohorts.unique(),columns=all_months )\n",
    "for cohort_month incohorts.unique():\n",
    "cohort_customers =cohorts[cohorts ==cohort_month ].index\n",
    "for monthinall_months :\n",
    "# Count customers from this cohort who purchased in this month\n",
    "retained_count =online_sales_df [(online_sales_df ['CustomerID' ].isin(cohort_customers\n",
    "# Total customers in the cohort\n",
    "total_cohort_customers =len(cohort_customers )\n",
    "iftotal_cohort_customers >0:\n",
    "retention_matrix .loc[cohort_month ,month]=retained_count /total_cohort_customers\n",
    "else :\n",
    "retention_matrix .loc[cohort_month ,month]=0\n",
    "# Fill NaN values with 0 where a cohort did not exist in a given month\n",
    "retention_matrix =retention_matrix .fillna(0)\n",
    "print(\"Cohort Retention Matrix (%):\" )\n",
    "display(retention_matrix )\n",
    "# Calculate overall retention rate for each cohort (excluding acquisition month)\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd212bf",
   "metadata": {},
   "source": [
    "## Page 33\n",
    "\n",
    "# Find the index of the acquisition month for each cohort\n",
    "acquisition_month_indices ={month:all_months .index(month)for monthinretention_matrix\n",
    "# Calculate the average retention rate for each cohort, excluding the acquisition month\n",
    "average_cohort_retention ={}\n",
    "for cohort_month inretention_matrix .index:\n",
    "acquisition_index =acquisition_month_indices [cohort_month ]\n",
    "# Consider months after the acquisition month\n",
    "post_acquisition_months =all_months [acquisition_index +1:]\n",
    "ifpost_acquisition_months :\n",
    "# Calculate average retention rate for these months\n",
    "average_retention =retention_matrix .loc[cohort_month ,post_acquisition_months\n",
    "average_cohort_retention [cohort_month ]=average_retention\n",
    "else :\n",
    "average_cohort_retention [cohort_month ]=0# No subsequent months to calculate retention\n",
    "average_cohort_retention_series =pd.Series(average_cohort_retention ).sort_index\n",
    "print(\"\\nAverage Retention Rate for Each Cohort (Excluding Acquisition Month):\"\n",
    "display(average_cohort_retention_series )\n",
    "highest_retention_cohort =average_cohort_retention_series .idxmax()\n",
    "lowest_retention_cohort =average_cohort_retention_series .idxmin()\n",
    "print(f\"\\nCohort with Highest Average Retention: {highest_retention_cohort }({average_cohort_retention_series\n",
    "print(f\"Cohort with Lowest Average Retention: {lowest_retention_cohort }({average_cohort_retention_series\n",
    "Cohort Retention Matrix (%):\n",
    "/tmp/ipython-input-2742354238.py:19: FutureWarning: Downcasting object dtype ar\n",
    "rays on .fillna, .ffill, .bfill is deprecated and will change in a future versi\n",
    "on. Call result.infer_objects(copy=False) instead. To opt-in to the future beha\n",
    "vior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
    "retention_matrix = retention_matrix.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc07be13",
   "metadata": {},
   "source": [
    "## Page 34\n",
    "\n",
    "2019-01 2019-02 2019-03 2019-04 2019-05 2019-06 2019-07\n",
    "2019-09 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
    "2019-03 0.0 0.000000 100.000000 10.169492 19.774011 14.124294 18.079096\n",
    "2019-06 0.0 0.000000 0.000000 0.000000 0.000000 100.000000 14.598540\n",
    "2019-12 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
    "2019-08 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
    "2019-05 0.0 0.000000 0.000000 0.000000 100.000000 10.714286 8.035714\n",
    "2019-07 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 100.000000\n",
    "2019-02 0.0 100.000000 7.291667 9.375000 16.666667 17.708333 22.916667\n",
    "2019-10 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
    "2019-04 0.0 0.000000 0.000000 100.000000 8.588957 14.723926 14.723926\n",
    "2019-01 100.0 6.046512 11.162791 15.813953 10.697674 20.465116 16.279070\n",
    "2019-11 0.0 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000\n",
    "Average Retention Rate for Each Cohort (Excluding Acquisition Month):\n",
    "0\n",
    "2019-01 13.742072\n",
    "2019-02 15.000000\n",
    "2019-03 13.873195\n",
    "2019-04 10.199387\n",
    "2019-05 10.076531\n",
    "2019-06 10.948905\n",
    "2019-07 9.148936\n",
    "2019-08 8.703704\n",
    "2019-09 4.700855\n",
    "2019-10 5.747126\n",
    "2019-11 10.294118\n",
    "2019-12 0.000000\n",
    "dtype: float64\n",
    "Cohort with Highest Average Retention: 2019-02 (15.00%)\n",
    "Cohort with Lowest Average Retention: 2019-12 (0.00%)\n",
    "print(\"\\nStrategies to improve retention for weaker cohorts (e.g., 2019-12 cohort with 0\n",
    "print(\"- **Win-back Campaigns:** For the 2019-12 cohort, as they haven't returned after their acquisition month, implement aggressive win-back campaigns with compelling offers or incentives to encourage a second purchase.\"\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e12cb",
   "metadata": {},
   "source": [
    "## Page 35\n",
    "\n",
    "print(\"- **Targeted Engagement:** For cohorts with low but not zero retention (e.g., 2019-09, 2019-10), analyze their initial purchase behavior (product category, price point) and send targeted communications with relevant product recommendations or discounts.\"\n",
    "print(\"- **Collect Feedback:** Reach out to customers in these weaker cohorts to understand why they haven't returned. This could be through surveys, emails, or customer service interactions. Use this feedback to address pain points and improve the customer experience.\"\n",
    "print(\"- **Loyalty Program Introduction:** Introduce or promote a loyalty program that rewards repeat purchases, even small ones, to incentivize customers in these cohorts to return.\"\n",
    "print(\"- **Improve Onboarding:** For newer weaker cohorts (like 2019-12), review the initial onboarding experience. Was it smooth? Did they receive timely follow-up communication? Improving the initial post-purchase experience can be crucial for early retention.\"\n",
    "print(\"- **Analyze External Factors:** Consider if there were any external factors or events in the months corresponding to these weaker cohorts (e.g., September, October, December 2019) that might have impacted their return behavior (e.g., competitor promotions, economic conditions).\"\n",
    "Strategies to improve retention for weaker cohorts (e.g., 2019-12 cohort with\n",
    "0% average retention, 2019-09 and 2019-10 cohorts with low retention):\n",
    "- **Win-back Campaigns:** For the 2019-12 cohort, as they haven't returned afte\n",
    "r their acquisition month, implement aggressive win-back campaigns with compell\n",
    "ing offers or incentives to encourage a second purchase.\n",
    "- **Targeted Engagement:** For cohorts with low but not zero retention (e.g., 2\n",
    "019-09, 2019-10), analyze their initial purchase behavior (product category, pr\n",
    "ice point) and send targeted communications with relevant product recommendatio\n",
    "ns or discounts.\n",
    "- **Collect Feedback:** Reach out to customers in these weaker cohorts to under\n",
    "stand why they haven't returned. This could be through surveys, emails, or cust\n",
    "omer service interactions. Use this feedback to address pain points and improve\n",
    "the customer experience.\n",
    "- **Loyalty Program Introduction:** Introduce or promote a loyalty program that\n",
    "rewards repeat purchases, even small ones, to incentivize customers in these co\n",
    "horts to return.\n",
    "- **Improve Onboarding:** For newer weaker cohorts (like 2019-12), review the i\n",
    "nitial onboarding experience. Was it smooth? Did they receive timely follow-up\n",
    "communication? Improving the initial post-purchase experience can be crucial fo\n",
    "r early retention.\n",
    "- **Analyze External Factors:** Consider if there were any external factors or\n",
    "events in the months corresponding to these weaker cohorts (e.g., September, Oc\n",
    "tober, December 2019) that might have impacted their return behavior (e.g., com\n",
    "petitor promotions, economic conditions).\n",
    "Business question 13\n",
    "Analyze the lifetime value of customers acquir ed in differ ent months and suggest\n",
    "how this insight infor ms acquisition and r etention strategies.\n",
    "# 1. Calculate the lifetime value (LTV) for each customer (using Total_Customer_Revenue as a proxy)\n",
    "# The 'rfm_revenue_df' already contains 'Total_Customer_Revenue' for each customer\n",
    "# 2. Merge the LTV data with the first_purchase_df\n",
    "ltv_acquisition_df =pd.merge(rfm_revenue_df [['CustomerID' ,'Total_Customer_Revenue'\n",
    "first_purchase_df [['CustomerID' ,'Acquisition_Month'\n",
    "on='CustomerID' ,\n",
    "how='left')\n",
    "# 3. Calculate the average LTV for customers acquired in each month\n",
    "average_ltv_by_acquisition_month =ltv_acquisition_df .groupby('Acquisition_Month'\n",
    "# 4. Identify the acquisition months with the highest and lowest average LTV\n",
    "highest_ltv_month =average_ltv_by_acquisition_month .loc[average_ltv_by_acquisition_month\n",
    "lowest_ltv_month =average_ltv_by_acquisition_month .loc[average_ltv_by_acquisition_month\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c410d79",
   "metadata": {},
   "source": [
    "## Page 36\n",
    "\n",
    "# 5. Display the average LTV by acquisition month\n",
    "print(\"Average Lifetime Value (LTV) by Customer Acquisition Month:\" )\n",
    "display(average_ltv_by_acquisition_month .sort_values (by='Acquisition_Month' ))\n",
    "print(f\"\\nAcquisition Month with Highest Average LTV:\" )\n",
    "display(highest_ltv_month )\n",
    "print(f\"\\nAcquisition Month with Lowest Average LTV:\" )\n",
    "display(lowest_ltv_month )\n",
    "Average Lifetime Value (LTV) by Customer Acquisition Month:\n",
    "Acquisition_Month Total_Customer_Revenue\n",
    "0 2019-01 5144.576677\n",
    "1 2019-02 5742.918483\n",
    "2 2019-03 3673.926931\n",
    "3 2019-04 2904.905151\n",
    "4 2019-05 3028.219681\n",
    "5 2019-06 1993.055264\n",
    "6 2019-07 2682.016046\n",
    "7 2019-08 1941.382743\n",
    "8 2019-09 1758.703938\n",
    "9 2019-10 2910.516153\n",
    "10 2019-11 3062.524806\n",
    "11 2019-12 1964.829619\n",
    "Acquisition Month with Highest Average LTV:\n",
    "1\n",
    "Acquisition_Month 2019-02\n",
    "Total_Customer_Revenue 5742.918483\n",
    "dtype: object\n",
    "Acquisition Month with Lowest Average LTV:\n",
    "8\n",
    "Acquisition_Month 2019-09\n",
    "Total_Customer_Revenue 1758.703938\n",
    "dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236369d",
   "metadata": {},
   "source": [
    "## Page 37\n",
    "\n",
    "Reasoning : Suggest strategies based on the identified L TV by acquisition month to\n",
    "inform acquisition and r etention efforts.\n",
    "print(\"\\nStrategies based on LTV by Acquisition Month Analysis:\" )\n",
    "print(\"- **Focus Acquisition Efforts:** The analysis shows that customers acquired in February 2019 have the highest average LTV. Investigate the marketing channels, campaigns, and strategies that were particularly active or successful in acquiring customers during February 2019. Allocate more resources to these high-LTV acquisition sources in the future.\"\n",
    "print(\"- **Analyze Low LTV Acquisition Months:** Customers acquired in September 2019 have the lowest average LTV. Analyze the acquisition sources and campaigns from September 2019. Identify potential issues with the quality of leads or the effectiveness of the messaging in attracting high-value customers. Consider adjusting or reducing investment in these low-LTV acquisition channels.\"\n",
    "print(\"- **Tailor Retention Strategies:** Understand the characteristics and behaviors of customers acquired in high-LTV months versus low-LTV months. Develop tailored retention strategies for different acquisition cohorts. For example, cohorts with higher initial LTV might respond well to loyalty programs and exclusive offers, while cohorts with lower initial LTV might need more targeted engagement and incentives to increase their spending over time.\"\n",
    "print(\"- **Monitor LTV Trends:** Continuously monitor LTV by acquisition month over time. This will help track the effectiveness of changes in acquisition and retention strategies and identify new trends in customer value.\"\n",
    "print(\"- **Analyze Product/Category Preferences:** Investigate if customers acquired in high-LTV months tend to purchase specific product categories or higher-value products. Use this information to inform product recommendations and marketing efforts for newer cohorts.\"\n",
    "Strategies based on LTV by Acquisition Month Analysis:\n",
    "- **Focus Acquisition Efforts:** The analysis shows that customers acquired in\n",
    "February 2019 have the highest average LTV. Investigate the marketing channels,\n",
    "campaigns, and strategies that were particularly active or successful in acquir\n",
    "ing customers during February 2019. Allocate more resources to these high-LTV a\n",
    "cquisition sources in the future.\n",
    "- **Analyze Low LTV Acquisition Months:** Customers acquired in September 2019\n",
    "have the lowest average LTV. Analyze the acquisition sources and campaigns from\n",
    "September 2019. Identify potential issues with the quality of leads or the effe\n",
    "ctiveness of the messaging in attracting high-value customers. Consider adjusti\n",
    "ng or reducing investment in these low-LTV acquisition channels.\n",
    "- **Tailor Retention Strategies:** Understand the characteristics and behaviors\n",
    "of customers acquired in high-LTV months versus low-LTV months. Develop tailore\n",
    "d retention strategies for different acquisition cohorts. For example, cohorts\n",
    "with higher initial LTV might respond well to loyalty programs and exclusive of\n",
    "fers, while cohorts with lower initial LTV might need more targeted engagement\n",
    "and incentives to increase their spending over time.\n",
    "- **Monitor LTV Trends:** Continuously monitor LTV by acquisition month over ti\n",
    "me. This will help track the effectiveness of changes in acquisition and retent\n",
    "ion strategies and identify new trends in customer value.\n",
    "- **Analyze Product/Category Preferences:** Investigate if customers acquired i\n",
    "n high-LTV months tend to purchase specific product categories or higher-value\n",
    "products. Use this information to inform product recommendations and marketing\n",
    "efforts for newer cohorts.\n",
    "Business question 14\n",
    "Identif y seasonal tr ends in sales by category and location and suggest how to\n",
    "prepare for peak and off-peak seasons.\n",
    "merged_df =pd.merge(merged_df ,customers_df [['CustomerID' ,'Location' ]],on='CustomerID'\n",
    "merged_df ['Month']=merged_df ['Transaction_Date_transaction' ].dt.month\n",
    "merged_df ['Day_of_Week' ]=merged_df ['Transaction_Date_transaction' ].dt.dayofweek\n",
    "seasonal_trends_category_location =merged_df .groupby(['Month','Product_Category'\n",
    "print(\"Seasonal Sales Trends by Product Category and Location:\" )\n",
    "display(seasonal_trends_category_location )\n",
    "Seasonal Sales Trends by Product Category and Location:\n",
    "In[]:\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a81a63",
   "metadata": {},
   "source": [
    "## Page 38\n",
    "\n",
    "Month Product_Category Location Invoice_V alue\n",
    "0 1 Accessories Chicago 57.55430\n",
    "1 1 Android Califor nia 49.74880\n",
    "2 1 Android Chicago 22.83500\n",
    "3 1 Android New Y ork 26.91380\n",
    "4 1 Appar el Califor nia 21147.12484\n",
    "... ... ... ... ...\n",
    "885 12 Waze Califor nia 239.00850\n",
    "886 12 Waze Chicago 433.11456\n",
    "887 12 Waze New Jersey 105.24612\n",
    "888 12 Waze New Y ork 129.33686\n",
    "889 12 Waze Washington DC 60.46200\n",
    "890 r ows  4 columns\n",
    "# Identify months/periods that show consistently high sales across categories and locations (peak seasons)\n",
    "# and those that show consistently low sales (off-peak seasons).\n",
    "# A simple approach is to look at the total revenue per month.\n",
    "monthly_total_revenue =merged_df .groupby('Month')['Invoice_Value' ].sum().reset_index\n",
    "print(\"\\nTotal Monthly Revenue:\" )\n",
    "display(monthly_total_revenue .sort_values (by='Month'))\n",
    "# Identify peak and off-peak months based on total monthly revenue\n",
    "peak_months =monthly_total_revenue .nlargest (3,'Invoice_Value' )['Month'].tolist\n",
    "off_peak_months =monthly_total_revenue .nsmallest (3,'Invoice_Value' )['Month'].\n",
    "print(f\"\\nIdentified Peak Season Months (based on total revenue): {peak_months }\n",
    "print(f\"Identified Off-Peak Season Months (based on total revenue): {off_peak_months\n",
    "print(\"\\nSuggested Strategies based on Seasonal Trends:\" )\n",
    "print(\"\\n**Preparing for Peak Seasons (e.g., Months {}:**\".format(peak_months ))\n",
    "print(\"- **Inventory Management:** Increase inventory levels for all product categories, especially for top-performing products identified earlier, well in advance of these months to avoid stockouts.\"\n",
    "print(\"- **Staffing:** Ensure adequate staffing levels in warehouses and customer service to handle increased order volume and inquiries.\"\n",
    "print(\"- **Marketing:** Plan and execute targeted marketing campaigns to capitalize on the increased demand. Highlight popular products and consider limited-time offers.\"\n",
    "print(\"- **Supply Chain:** Work closely with suppliers to ensure timely delivery of goods during peak periods.\"\n",
    "print(\"\\n**Preparing for Off-Peak Seasons (e.g., Months {}:**\".format(off_peak_months\n",
    "print(\"- **Inventory Management:** Implement strategies to manage excess inventory, such as running promotions, offering discounts, or bundling products.\"\n",
    "print(\"- **Marketing:** Focus on customer retention strategies and targeted promotions to stimulate demand. Explore new marketing channels or campaigns that might be more effective during slower periods.\"\n",
    "print(\"- **Operational Efficiency:** Analyze operational costs and identify areas for efficiency improvements during slower months.\"\n",
    "print(\"- **Planning:** Use off-peak periods for planning and preparing for the next peak season.\"\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e6523",
   "metadata": {},
   "source": [
    "## Page 39\n",
    "\n",
    "print(\"\\n**Strategies by Category and Location:**\" )\n",
    "print(\"- Analyze the `seasonal_trends_category_location` DataFrame to identify specific product categories and locations that have significant peaks or dips in sales within the identified peak and off-peak months. Tailor inventory and promotional strategies for these specific combinations.\"\n",
    "print(\"- For categories/locations with significant seasonal fluctuations, consider localized marketing campaigns and inventory adjustments.\"\n",
    "Total Monthly Revenue:\n",
    "Month Invoice_V alue\n",
    "0 1463883.05705\n",
    "1 2327896.56020\n",
    "2 3336805.20383\n",
    "3 4447999.19523\n",
    "4 5318556.30056\n",
    "5 6289830.32931\n",
    "6 7423982.34361\n",
    "7 8418160.56704\n",
    "8 9321128.35638\n",
    "9 10 450837.46255\n",
    "10 11 475902.15336\n",
    "11 12 439530.03015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558e179",
   "metadata": {},
   "source": [
    "## Page 40\n",
    "\n",
    "Identified Peak Season Months (based on total revenue): [11, 1, 10]\n",
    "Identified Off-Peak Season Months (based on total revenue): [6, 5, 9]\n",
    "Suggested Strategies based on Seasonal Trends:\n",
    "**Preparing for Peak Seasons (e.g., Months [11, 1, 10]:**\n",
    "- **Inventory Management:** Increase inventory levels for all product categorie\n",
    "s, especially for top-performing products identified earlier, well in advance o\n",
    "f these months to avoid stockouts.\n",
    "- **Staffing:** Ensure adequate staffing levels in warehouses and customer serv\n",
    "ice to handle increased order volume and inquiries.\n",
    "- **Marketing:** Plan and execute targeted marketing campaigns to capitalize on\n",
    "the increased demand. Highlight popular products and consider limited-time offe\n",
    "rs.\n",
    "- **Supply Chain:** Work closely with suppliers to ensure timely delivery of go\n",
    "ods during peak periods.\n",
    "**Preparing for Off-Peak Seasons (e.g., Months [6, 5, 9]:**\n",
    "- **Inventory Management:** Implement strategies to manage excess inventory, su\n",
    "ch as running promotions, offering discounts, or bundling products.\n",
    "- **Marketing:** Focus on customer retention strategies and targeted promotions\n",
    "to stimulate demand. Explore new marketing channels or campaigns that might be\n",
    "more effective during slower periods.\n",
    "- **Operational Efficiency:** Analyze operational costs and identify areas for\n",
    "efficiency improvements during slower months.\n",
    "- **Planning:** Use off-peak periods for planning and preparing for the next pe\n",
    "ak season.\n",
    "**Strategies by Category and Location:**\n",
    "- Analyze the `seasonal_trends_category_location` DataFrame to identify specifi\n",
    "c product categories and locations that have significant peaks or dips in sales\n",
    "within the identified peak and off-peak months. Tailor inventory and promotiona\n",
    "l strategies for these specific combinations.\n",
    "- For categories/locations with significant seasonal fluctuations, consider loc\n",
    "alized marketing campaigns and inventory adjustments.\n",
    "Business question 15\n",
    "Analyze daily sales tr ends to identif y high-perfor ming and low-perfor ming days and\n",
    "suggest strategies to boost sales on slower days.\n",
    "# 1. Group by 'Transaction_Date_transaction' and calculate daily revenue\n",
    "daily_sales =merged_df .groupby('Transaction_Date_transaction' )['Invoice_Value'\n",
    "daily_sales .rename(columns={'Invoice_Value' :'Total_Daily_Revenue' },inplace=True\n",
    "# 2. Identify dates with the highest and lowest total daily revenue\n",
    "highest_revenue_day =daily_sales .loc[daily_sales ['Total_Daily_Revenue' ].idxmax\n",
    "lowest_revenue_day =daily_sales .loc[daily_sales ['Total_Daily_Revenue' ].idxmin()]\n",
    "# 3. Extract the day of the week\n",
    "daily_sales ['Day_of_Week_Name' ]=daily_sales ['Transaction_Date_transaction' ].dt\n",
    "In[]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537a4b0",
   "metadata": {},
   "source": [
    "## Page 41\n",
    "\n",
    "# 4. Calculate the average daily revenue for each day of the week\n",
    "average_daily_revenue_by_weekday =daily_sales .groupby('Day_of_Week_Name' )['Total_Daily_Revenue'\n",
    "# 5. Identify the days of the week with the highest and lowest average sales\n",
    "highest_avg_day =average_daily_revenue_by_weekday .idxmax()\n",
    "lowest_avg_day =average_daily_revenue_by_weekday .idxmin()\n",
    "# 6. Print the findings\n",
    "print(\"Daily Sales Analysis:\" )\n",
    "print(\"\\nDate with Highest Daily Revenue:\" )\n",
    "display(highest_revenue_day )\n",
    "print(\"\\nDate with Lowest Daily Revenue:\" )\n",
    "display(lowest_revenue_day )\n",
    "print(\"\\nAverage Daily Revenue by Day of the Week:\" )\n",
    "display(average_daily_revenue_by_weekday )\n",
    "print(f\"\\nDay of the Week with Highest Average Sales: {highest_avg_day }({average_daily_revenue_by_weekday\n",
    "print(f\"Day of the Week with Lowest Average Sales: {lowest_avg_day }({average_daily_revenue_by_weekday\n",
    "# 7. Suggest strategies\n",
    "print(\"\\nSuggested Strategies to Boost Sales on Slower Days:\" )\n",
    "print(f\"- **Targeted Promotions on {lowest_avg_day }s:** Run special promotions, discounts, or limited-time offers specifically on\n",
    "print(f\"- **Analyze Customer Behavior:** Investigate customer behavior on {lowest_avg_day\n",
    "print(f\"- **Timed Marketing Campaigns:** Schedule email marketing campaigns or social media posts to go out on\n",
    "print(f\"- **Flash Sales or Exclusive Deals:** Introduce short-duration flash sales or exclusive deals that are only available on\n",
    "print(f\"- **Content Marketing:** Publish engaging content (blog posts, social media content, videos) on\n",
    "print(f\"- **Analyze the Lowest Performing Individual Day ( {lowest_revenue_day ['Transaction_Date_transaction'\n",
    "print(\"- **Consider Free Shipping or Reduced Delivery Charges:** Offering free shipping or reduced delivery charges on slower days can remove a potential barrier to purchase.\"\n",
    "Daily Sales Analysis:\n",
    "Date with Highest Daily Revenue:\n",
    "94\n",
    "Transaction_Date_transaction 2019-04-05 00:00:00\n",
    "Total_Daily_Revenue 56753.00369\n",
    "dtype: object\n",
    "Date with Lowest Daily Revenue:\n",
    "147\n",
    "Transaction_Date_transaction 2019-05-28 00:00:00\n",
    "Total_Daily_Revenue 1767.15072\n",
    "dtype: object\n",
    "Average Daily Revenue by Day of the Week:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe088018",
   "metadata": {},
   "source": [
    "## Page 42\n",
    "\n",
    "Total_Daily_Revenue\n",
    "Day_of_W eek_Name\n",
    "Monda y 6858.989679\n",
    "Tuesda y 7419.297182\n",
    "Wednesda y 16088.606616\n",
    "Thursda y 16443.641897\n",
    "Friday 16773.351936\n",
    "Saturda y 13324.793552\n",
    "Sunda y 13612.324178\n",
    "dtype: float64\n",
    "Day of the Week with Highest Average Sales: Friday (16773.35)\n",
    "Day of the Week with Lowest Average Sales: Monday (6858.99)\n",
    "Suggested Strategies to Boost Sales on Slower Days:\n",
    "- **Targeted Promotions on Mondays:** Run special promotions, discounts, or lim\n",
    "ited-time offers specifically on Mondays to incentivize purchases and drive tra\n",
    "ffic.\n",
    "- **Analyze Customer Behavior:** Investigate customer behavior on Mondays compa\n",
    "red to higher-performing days. Are customers less likely to browse or purchase?\n",
    "Are there specific product categories that perform poorly? Use these insights t\n",
    "o tailor strategies.\n",
    "- **Timed Marketing Campaigns:** Schedule email marketing campaigns or social m\n",
    "edia posts to go out on Mondays or the day before, highlighting promotions or n\n",
    "ew arrivals to encourage shopping on the slower day.\n",
    "- **Flash Sales or Exclusive Deals:** Introduce short-duration flash sales or e\n",
    "xclusive deals that are only available on Mondays to create urgency and encoura\n",
    "ge immediate purchases.\n",
    "- **Content Marketing:** Publish engaging content (blog posts, social media con\n",
    "tent, videos) on Mondays that drives traffic to the website, even if it doesn't\n",
    "immediately result in a purchase, it keeps the brand top-of-mind.\n",
    "- **Analyze the Lowest Performing Individual Day (2019-05-28):** Investigate if\n",
    "there were any specific issues on this date (e.g., website downtime, technical\n",
    "glitches, external events) that might have caused the unusually low sales and a\n",
    "ddress them.\n",
    "- **Consider Free Shipping or Reduced Delivery Charges:** Offering free shippin\n",
    "g or reduced delivery charges on slower days can remove a potential barrier to\n",
    "purchase."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
